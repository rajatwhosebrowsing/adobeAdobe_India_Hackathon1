# Adobe India Hackathon â€“ Document Intelligence RAG Pipeline

## ğŸ“Œ Overview
This project is a **Retrieval-Augmented Generation (RAG)** pipeline built for the Adobe India Hackathon.  
It processes PDF documents, retrieves relevant sections based on a specific job description or persona, and generates **structured JSON summaries** ranked by importance.

It uses:
- **Ollama** for running a custom fine-tuned LLM (`hackathon`)
- **LangChain** for prompt handling, document processing, and orchestration
- **HuggingFace Embeddings** (`all-MiniLM-L6-v2`) + **FAISS** for semantic search
- **Docker** for containerized deployment

---

## ğŸš€ Features
- ğŸ“‚ **Multi-PDF ingestion** â€“ processes all PDFs from `/app/input`
- ğŸ§  **Custom local LLM** (`hackathon`) created from a `Modelfile` + `.gguf` weights
- ğŸ” **Semantic search** with FAISS vector store
- ğŸ“Š **Importance ranking** for each extracted section
- ğŸ“ Outputs **structured JSON** with metadata, source document, page number, and ranked summaries
- ğŸ³ Fully **Dockerized** for easy deployment

---

## ğŸ› ï¸ Tech Stack
- **Python 3.11**
- **Ollama** (Custom model creation & inference)
- **LangChain**
- **HuggingFace Embeddings**
- **FAISS** (vector database)
- **PyPDFLoader** for PDF parsing
- **Docker (multi-stage build)**

---

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ final1.py              # Main RAG processing script
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # Multi-stage Docker build (Python + Ollama)
â”œâ”€â”€ Modelfile              # Ollama model definition
â”œâ”€â”€ model.gguf             # LLM weights (DeepSeek-R1 Distill Qwen 1.5B)
â”œâ”€â”€ /input                  # Place PDFs and config.json here
â”œâ”€â”€ /output                 # Processed JSON output
```

---

## âš™ï¸ Installation & Usage

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/rajatwhosebrowsing/adobeAdobe_India_Hackathon1.git
cd adobeAdobe_India_Hackathon1
```

### 2ï¸âƒ£ Prepare input
Inside the `/input` folder, place:
- One or more `.pdf` files
- A `config.json` file in the format:
```json
{
  "persona": "Senior Data Analyst",
  "job_to_be_done": "Identify key financial metrics and risk indicators"
}
```

### 3ï¸âƒ£ Build Docker image
```bash
docker build -t hackathon-rag .
```

### 4ï¸âƒ£ Run the container
```bash
docker run --rm -v $(pwd)/input:/app/input -v $(pwd)/output:/app/output hackathon-rag
```

The processed `result.json` will be in `/output`.

---

## ğŸ–¥ï¸ How It Works
1. **Load PDFs & Config** â†’ Reads persona & job target from `config.json`
2. **Split Text** â†’ Splits documents into overlapping chunks
3. **Embed & Store** â†’ Creates FAISS vector store with HuggingFace embeddings
4. **Retrieve** â†’ Gets top-k chunks relevant to the job
5. **Analyze** â†’ Uses `hackathon` LLM to output structured JSON with:
   - `section_title`
   - `importance_rank`
   - `refined_text` summary
6. **Save** â†’ Writes output to `/output/result.json`

---

## ğŸ“¦ Dependencies
Install locally (for development without Docker):
```bash
pip install -r requirements.txt
```

---

## ğŸ“ Example Output
```json
{
  "metadata": {
    "input_documents": ["report1.pdf"],
    "persona": "Senior Data Analyst",
    "job_to_be_done": "Identify key financial metrics and risk indicators",
    "processing_timestamp": "2025-08-10T14:00:00"
  },
  "Extracted Section": [
    {
      "section_title": "Q2 Financial Overview",
      "importance_rank": 1,
      "refined_text": "Summarized analysis here...",
      "document": "report1.pdf",
      "page_number": 5
    }
  ]
}
```

---

## ğŸ§ª Testing
Run locally:
```bash
python3 final1.py
```

Make sure you have:
- `/input` folder with PDFs and `config.json`
- Ollama running locally with `hackathon` model loaded

---

## ğŸ“œ License
MIT License Â© 2025 Rajat
